{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from aria.utils import _load_weight\n",
    "from aria.config import load_model_config\n",
    "from aria.tokenizer import AbsTokenizer, SeparatedAbsTokenizer\n",
    "from src.load_aria_weights import get_p2q\n",
    "import argparse\n",
    "import torch\n",
    "from torch.nn.functional import log_softmax\n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from aria.data.midi import MidiDict\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import load_config\n",
    "\n",
    "\n",
    "M_PATH = \"./inference/weights/model.safetensors\"\n",
    "tokenizer = AbsTokenizer()\n",
    "tokenizer.vocab_size\n",
    "tokenizer.add_tokens_to_vocab(['<SEP>'])\n",
    "config = load_config(\"train_config.json\")\n",
    "model = get_p2q(config, tokenizer, M_PATH, False)\n",
    "# model = get_p2q(config, tokenizer)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_greedy_search(model, enc_input, max_length, device):\n",
    "\n",
    "    start_token = tokenizer.encode(['<S>'])[0]\n",
    "    end_token = tokenizer.encode(['<E>'])[0]\n",
    "    pad_token = tokenizer.encode(['<P>'])[0]\n",
    "    \n",
    "    sequences = [start_token]  # Initialize with the start token\n",
    "\n",
    "    for pos in range(max_length):\n",
    "        chunk = enc_input[pos:pos + 1096]\n",
    "        \n",
    "        if len(chunk) < max_length:\n",
    "            # Padding the last chunk if it's smaller than the window size\n",
    "            chunk = chunk + [pad_token] * (max_length - len(chunk))\n",
    "        \n",
    "        chunk_tensor = torch.tensor(chunk, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        encoder_output = model.encode(chunk_tensor)\n",
    "\n",
    "        current_length = len(sequences)\n",
    "        padded_sequences = sequences + [pad_token] * (max_length - current_length)\n",
    "        \n",
    "        # Convert the sequence list into a tensor wiaath the correct shape\n",
    "        input_tensor = torch.tensor(padded_sequences, device=device).unsqueeze(0)  # Adding batch dimension\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            logits = model.logits(input_tensor, encoder_output)\n",
    "        \n",
    "        logits = logits[:, -1, :]  # Take the logits of the last token\n",
    "        next_token = torch.argmax(logits, dim=-1).item()  # Choose the token with the highest probability\n",
    "        \n",
    "        sequences.append(next_token)  # Append the chosen token to the sequence\n",
    "        \n",
    "        if next_token == end_token:  # Stop if the end token is generated\n",
    "            break\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "def inference(midi_path):\n",
    "    \n",
    "    midi_dict = MidiDict.from_midi(midi_path)\n",
    "    tokenized_midi = tokenizer._tokenize_midi_dict(midi_dict=midi_dict)\n",
    "    encoded_midi_seq = tokenizer.encode(tokenized_midi)\n",
    "    decoded_seq = single_greedy_search(model, encoded_midi_seq, 4096, 'cuda')\n",
    "    raw_output = tokenizer.decode(decoded_seq)\n",
    "    return tokenized_midi, raw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<S>', ('piano', 70, 45), ('onset', 0), ('dur', 4420), ('piano', 79, 45), ('onset', 950), ('dur', 3470), ('piano', 39, 30), ('onset', 970), ('dur', 1740), ('piano', 63, 45), ('onset', 1610), ('dur', 1100), ('piano', 55, 30), ('onset', 1620), ('dur', 1090), ('piano', 67, 60), ('onset', 2140), ('dur', 570), ('piano', 58, 45), ('onset', 2150), ('dur', 560), ('piano', 63, 60), ('onset', 2170), ('dur', 540), ('piano', 51, 45), ('onset', 2720), ('dur', 1700), ('piano', 77, 60), ('onset', 3250), ('dur', 1170), ('piano', 62, 60), ('onset', 3260), ('dur', 1160), ('piano', 56, 45), ('onset', 3270), ('dur', 1150), ('piano', 68, 60), ('onset', 3780), ('dur', 640), ('piano', 59, 45), ('onset', 3810), ('dur', 610), ('piano', 79, 75), ('onset', 3810), ('dur', 3860), ('piano', 62, 45), ('onset', 3830), ('dur', 590), ('piano', 77, 60), ('onset', 4350), ('dur', 3320), ('piano', 39, 60), ('onset', 4370), ('dur', 1730), ('piano', 63, 60), ('onset', 4970), ('dur', 1130), ('piano', 55, 45), ('onset', 4990), ('dur', 1110), '<T>', ('piano', 63, 60), ('onset', 500), ('dur', 600), ('piano', 67, 45), ('onset', 510), ('dur', 590), ('piano', 58, 60), ('onset', 510), ('dur', 590), ('piano', 75, 45), ('onset', 1100), ('dur', 1570), ('piano', 38, 60), ('onset', 1110), ('dur', 1560), ('piano', 55, 45), ('onset', 1620), ('dur', 1050), ('piano', 63, 60), ('onset', 1630), ('dur', 1040), ('piano', 67, 45), ('onset', 2150), ('dur', 520), ('piano', 70, 75), ('onset', 2150), ('dur', 2150), ('piano', 58, 45), ('onset', 2160), ('dur', 510), ('piano', 63, 45), ('onset', 2170), ('dur', 500), ('piano', 36, 45), ('onset', 2660), ('dur', 1640), ('piano', 79, 75)]\n",
      "['<S>', ('piano', 61, 60), ('piano', 61, 60), ('piano', 61, 60), ('piano', 61, 60), ('piano', 61, 60), ('piano', 61, 60), ('piano', 61, 60), ('piano', 61, 60), ('piano', 61, 60), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70), ('dur', 70)]\n"
     ]
    }
   ],
   "source": [
    "midi_path = \"datasets/paired-dataset-5/performance/audio-https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DL2KdUIeYTQo.mid\"\n",
    "input, output = inference(midi_path)\n",
    "print(input[1:100])\n",
    "print(output[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
